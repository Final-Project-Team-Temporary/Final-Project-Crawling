from celery import Celery
from pymongo import MongoClient
from dotenv import load_dotenv
import json
import subprocess
import os

load_dotenv()

# 1. Celery ì„¤ì •
app = Celery('tasks', broker=os.getenv("REDIS_URL"))

# 2. MongoDB ì—°ê²°
mongo_url = os.getenv("MONGO_URL")
client = MongoClient(mongo_url)
db = client['ArticleDatabase']
collection = db['articles']

@app.task
def crawl_and_store_articles():
    print("ğŸš€ í¬ë¡¤ë§ ì‹œì‘: python naver_spider.py ì‹¤í–‰")

    # 3. ê¸°ì¡´ output.json ì‚­ì œ (ë®ì–´ì“°ê¸° ë°©ì§€)
    if os.path.exists("output.json"):

        os.remove("output.json")

    # 4. Scrapy í¬ë¡¤ëŸ¬ ì‹¤í–‰
    result = subprocess.run(
        ['python', 'naver_spider.py'],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True
    )

    if result.returncode != 0:
        print("âŒ í¬ë¡¤ë§ ì‹¤íŒ¨:")
        print(result.stderr)
        return

    # 5. output.json í™•ì¸
    if not os.path.exists("output.json"):
        print("âŒ output.json íŒŒì¼ ì—†ìŒ")
        return

    print("ğŸ“¦ output.json í™•ì¸ ì™„ë£Œ, MongoDB ì €ì¥ ì‹œì‘")

    # 6. JSON ë¼ì¸ë³„ ë¡œë”© ë° ì €ì¥
    new_count = 0
    with open("output.json", "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue
            try:
                article = json.loads(line)
                if not collection.find_one({"url": article.get("url")}):
                    collection.insert_one(article)
                    new_count += 1
            except json.JSONDecodeError as e:
                print(f"âŒ JSON ë””ì½”ë”© ì‹¤íŒ¨: {e}")


    print(f"âœ… ì €ì¥ ì™„ë£Œ: {new_count}ê±´ ì¶”ê°€ë¨")


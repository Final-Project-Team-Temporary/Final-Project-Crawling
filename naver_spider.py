import scrapy
from scrapy.crawler import CrawlerProcess
import time
import os
from dotenv import load_dotenv
from datetime import datetime

load_dotenv()


def format_date_for_spring(date_str):
    """
    ë‚ ì§œ ë¬¸ìì—´ì„ Spring LocalDateTimeì´ íŒŒì‹± ê°€ëŠ¥í•œ ISO 8601 í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    "2025-10-23 20:37:26" -> "2025-10-23T20:37:26"
    """
    if not date_str or date_str == "ë‚ ì§œ ì—†ìŒ":
        return None
    
    try:
        # ì—¬ëŸ¬ í˜•ì‹ì„ ì§€ì›í•˜ë„ë¡ íŒŒì‹± ì‹œë„
        # "2025-10-23 20:37:26" í˜•ì‹ ì²˜ë¦¬
        if " " in date_str and "T" not in date_str:
            # ê³µë°±ì„ Të¡œ ë³€í™˜
            formatted = date_str.replace(" ", "T")
            # íŒŒì‹± ê°€ëŠ¥í•œì§€ ê²€ì¦
            datetime.fromisoformat(formatted)
            return formatted
        # ì´ë¯¸ ISO í˜•ì‹ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜
        elif "T" in date_str:
            return date_str
        # ë‹¤ë¥¸ í˜•ì‹ì¸ ê²½ìš° datetimeìœ¼ë¡œ íŒŒì‹± í›„ ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        else:
            # ë‹¤ì–‘í•œ í˜•ì‹ ì‹œë„
            formats = [
                "%Y-%m-%d %H:%M:%S",
                "%Y-%m-%dT%H:%M:%S",
                "%Y-%m-%d %H:%M:%S.%f",
                "%Y-%m-%dT%H:%M:%S.%f"
            ]
            for fmt in formats:
                try:
                    dt = datetime.strptime(date_str, fmt)
                    return dt.isoformat().split('.')[0]  # ë°€ë¦¬ì´ˆ ì œê±°í•˜ê³  ë°˜í™˜
                except ValueError:
                    continue
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
            return date_str
    except Exception as e:
        print(f"âš ï¸ ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨: {date_str}, ì˜¤ë¥˜: {e}")
        return date_str


class NaverNewsSpider(scrapy.Spider):
    name = "naver_news"
    start_urls = ['https://news.naver.com/breakingnews/section/101/259']

    custom_settings = {
        "LOG_LEVEL": "INFO"
    }

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.start_time = time.time()
        self.count = 0
        self.max_articles = int(os.getenv("MAX_ARTICLES", "10"))
        self.max_crawl_time = int(os.getenv("MAX_CRAWL_TIME", "300"))  # 5ë¶„

    def parse(self, response):
        # ì‹œê°„ ì œí•œ í™•ì¸
        if time.time() - self.start_time > self.max_crawl_time:
            print(f"â° ì‹œê°„ ì œí•œ({self.max_crawl_time}ì´ˆ) ë„ë‹¬, í¬ë¡¤ë§ ì¢…ë£Œ")
            return
            
        links = response.css("ul.sa_list li.sa_item a.sa_text_title::attr(href)").getall()
        for link in links[:self.max_articles]:
            if not link.startswith("http"):
                link = "https://n.news.naver.com" + link
            yield scrapy.Request(link, callback=self.parse_article)

    def parse_article(self, response):
        # ì‹œê°„ ì œí•œ í™•ì¸
        if time.time() - self.start_time > self.max_crawl_time:
            print(f"â° ì‹œê°„ ì œí•œ({self.max_crawl_time}ì´ˆ) ë„ë‹¬, í¬ë¡¤ë§ ì¢…ë£Œ")
            return
            
        # ê¸°ì‚¬ ê°œìˆ˜ ì œí•œ í™•ì¸
        if self.count >= self.max_articles:
            print(f"ğŸ“Š ê¸°ì‚¬ ê°œìˆ˜ ì œí•œ({self.max_articles}ê°œ) ë„ë‹¬, í¬ë¡¤ë§ ì¢…ë£Œ")
            return
        
        if self.count == 0:
            print(f"\nğŸ•’ Start: {time.strftime('%X')}")
            print(f"ğŸ“Š ëª©í‘œ: {self.max_articles}ê°œ ê¸°ì‚¬, ì‹œê°„ ì œí•œ: {self.max_crawl_time}ì´ˆ")

        title = response.css(".media_end_head_headline").xpath("string()").get()
        content = response.css(".go_trans._article_content").xpath("string()").get()
        date = response.css(".media_end_head_info_datestamp_time._ARTICLE_DATE_TIME::attr(data-date-time)").get()
        
        # Spring LocalDateTime íŒŒì‹± ê°€ëŠ¥í•œ ISO 8601 í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        formatted_date = format_date_for_spring(date) if date else None

        print(f"\nğŸ“„ [{self.count + 1}/{self.max_articles}] {response.url}")
        print(f"ì œëª©: {title.strip() if title else 'ì—†ìŒ'}")
        print(f"ë³¸ë¬¸ ê¸¸ì´: {len(content.strip()) if content else 0}ì")
        if formatted_date:
            print(f"ë‚ ì§œ: {formatted_date}")

        self.count += 1
        
        # ì™„ë£Œ ì¡°ê±´ í™•ì¸
        if self.count >= self.max_articles:
            elapsed_time = round(time.time() - self.start_time, 3)
            print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ! {self.count}ê°œ ê¸°ì‚¬, ì†Œìš” ì‹œê°„: {elapsed_time}ì´ˆ")

        # âœ… ì—¬ê¸°ì„œ ê²°ê³¼ë¥¼ ë°˜í™˜í•´ì•¼ Scrapyê°€ ì €ì¥í•¨!
        yield {
            "title": title.strip() if title else "ì œëª© ì—†ìŒ",
            "content": content.strip() if content else "",
            "publishedAt": formatted_date if formatted_date else None,
            "url": response.url,
            "summary_status": "BEFORE_ENQUEUED"
        }


if __name__ == "__main__":
    output_path = os.getenv("OUTPUT_FILE_PATH", "output.json")
    process = CrawlerProcess(settings={
        "USER_AGENT": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "LOG_LEVEL": "INFO",
        "FEED_FORMAT": "jsonlines",
        "FEED_URI": output_path,
        "CONCURRENT_REQUESTS": 2,  # ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œ
        "DOWNLOAD_DELAY": 1,       # ìš”ì²­ ê°„ 1ì´ˆ ëŒ€ê¸°
        "DOWNLOAD_TIMEOUT": 10,    # 10ì´ˆ íƒ€ì„ì•„ì›ƒ
        "RETRY_TIMES": 2,          # ì¬ì‹œë„ 2íšŒ
        "ROBOTSTXT_OBEY": False    # robots.txt ë¬´ì‹œ (ì†ë„ í–¥ìƒ)
    })
    process.crawl(NaverNewsSpider)
    process.start()

